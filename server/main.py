import os
import sys
import numpy as np
import librosa
import soundfile as sf
import warnings
from scipy import signal
import pyrubberband as pyrb

# ğŸ”¥ V2 ë¹„íŠ¸ ì¶”ì¶œê¸°
from services.extract_beat_v2 import extract_best_loop_v2 as extract_best_loop

# ê¸°ì¡´ ì„œë¹„ìŠ¤ ëª¨ë“ˆë“¤
from services.analyzer_beat import get_beat_info
from services.analyzer_intro import get_intro_duration
from services.analyzer_outro import find_outro_endpoint
from services.stem_separation import separate_stems
from services.analyzer_vocal import find_vocal_end_point
from services.analyzer_key import get_key_from_audio, get_pitch_shift_steps

# Madmom (Downbeat Snapìš©)
try:
    from madmom.features.downbeats import RNNDownBeatProcessor, DBNDownBeatTrackingProcessor
except ImportError:
    pass

warnings.filterwarnings("ignore")

# ====================================================
# ğŸ“ [ì„¤ì • ì˜ì—­]
# ====================================================
INPUT_DIR = "./uploads"
OUTPUT_DIR = "./output"
TRACK_A_NAME = "My Way.mp3"
TRACK_B_NAME = "Whiplash.mp3"
TARGET_SR = 44100
BPM_THRESHOLD = 20  # ğŸ”¥ BPM ì°¨ì´ê°€ ì´ ê°’ë³´ë‹¤ í¬ë©´ Drop Mix ì‹¤í–‰

# ====================================================
# ğŸ› ï¸ Helper Functions
# ====================================================

def normalize_audio(y, target_db=-1.0):
    max_val = np.max(np.abs(y))
    if max_val == 0: return y
    target_amp = 10 ** (target_db / 20)
    return y * (target_amp / max_val)

def preserve_energy(y_original, y_stretched):
    rms_orig = np.sqrt(np.mean(y_original**2))
    rms_new = np.sqrt(np.mean(y_stretched**2))
    if rms_new < 1e-5: return y_stretched
    gain = rms_orig / rms_new
    if gain > 3.0: gain = 3.0
    return y_stretched * gain

def smooth_concatenate(arrays, fade_samples=512):
    if not arrays: return np.array([])
    if len(arrays) == 1: return arrays[0]
    
    result = arrays[0]
    for i in range(1, len(arrays)):
        next_arr = arrays[i]
        
        if len(result) < fade_samples or len(next_arr) < fade_samples:
            result = np.concatenate([result, next_arr])
            continue
            
        fade_out = np.cos(np.linspace(0, np.pi / 2, fade_samples))
        fade_in = np.sin(np.linspace(0, np.pi / 2, fade_samples))
        
        overlap_prev = result[-fade_samples:] * fade_out
        overlap_next = next_arr[:fade_samples] * fade_in
        
        combined = overlap_prev + overlap_next
        result = np.concatenate([result[:-fade_samples], combined, next_arr[fade_samples:]])
        
    return result

def get_low_freq_energy(y, sr):
    """150Hz ì´í•˜ í‚¥/ë² ì´ìŠ¤ ì—ë„ˆì§€ ì¸¡ì • (ìœ„ìƒ ê²€ì¦ìš©)"""
    try:
        sos = signal.butter(4, 150, 'lp', fs=sr, output='sos')
        y_low = signal.sosfilt(sos, y)
        return np.sqrt(np.mean(y_low**2))
    except:
        return 0

def find_smart_trim_point(y, sr, target_sample, bpm_hint):
    """Smart Snap & Phase Correction"""
    try:
        print(f"   ğŸ•µï¸ Analyzing trim point near {target_sample/sr:.2f}s...")
        start_sec = max(0, (target_sample / sr) - 10.0)
        end_sec = min(len(y) / sr, (target_sample / sr) + 10.0)
        y_cut = y[int(start_sec*sr):int(end_sec*sr)]
        
        y_proc = y_cut * 2.0 
        y_proc = np.sign(y_proc) * (np.abs(y_proc) ** 2)

        proc = RNNDownBeatProcessor()
        act = proc(y_proc)
        
        tracker = DBNDownBeatTrackingProcessor(
            beats_per_bar=[4], fps=100,
            min_bpm=bpm_hint*0.8, max_bpm=bpm_hint*1.2, transition_lambda=150
        )
        beats_info = tracker(act)
        downbeats = beats_info[beats_info[:, 1] == 1][:, 0]
        downbeat_samples = (downbeats * sr).astype(int) + int(start_sec * sr)
        
        candidates_prev = downbeat_samples[downbeat_samples <= target_sample]
        if len(candidates_prev) == 0: return target_sample
        
        chosen_point = candidates_prev[-1]
        
        # 1. ìœ„ìƒ ë³´ì •
        check_len = int(0.4 * sr)
        e1 = get_low_freq_energy(y[chosen_point : chosen_point + check_len], sr)
        samples_per_beat = int(sr * 60 / bpm_hint)
        point_beat3 = chosen_point + (samples_per_beat * 2)
        
        if point_beat3 + check_len < len(y):
            e3 = get_low_freq_energy(y[point_beat3 : point_beat3 + check_len], sr)
            if e3 > e1 * 1.3:
                print("      ğŸ”„ Trim Phase Fix: Shifted to real Downbeat (+2 beats)")
                chosen_point = point_beat3
        
        # 2. ì „ì§„ ìŠ¤ëƒ…
        dist_from_bar_start = target_sample - chosen_point
        bar_length = samples_per_beat * 4
        if dist_from_bar_start > (bar_length * 0.5):
            print("      â© Trim Resolution Fix: Extending to finish the bar (Forward Snap)")
            chosen_point += bar_length
            
        print(f"   ğŸ“ Smart Trim Point: {chosen_point/sr:.2f}s")
        return chosen_point

    except Exception as e:
        print(f"   âš ï¸ Smart Trim failed ({e}). Using original.")
        return target_sample

def match_bpm_with_safety_margin(y, sr, current_bpm, target_bpm, target_len_samples):
    if current_bpm == target_bpm:
        if len(y) > target_len_samples: return y[:target_len_samples]
        return y
    
    rate = target_bpm / current_bpm
    y_stretched = pyrb.time_stretch(y, sr, rate)
    y_stretched = preserve_energy(y, y_stretched)
    
    if len(y_stretched) > target_len_samples:
        return y_stretched[:target_len_samples]
    elif len(y_stretched) < target_len_samples:
        pad_len = target_len_samples - len(y_stretched)
        return np.pad(y_stretched, (0, pad_len))
    return y_stretched

def create_tempo_ramp(y, sr, start_bpm, end_bpm, base_bpm, steps=32):
    """
    [ìˆ˜ì • ì‚¬í•­]
    BPM ì¦ê°€ ê³¡ì„ ì„ 'Linear(ì„ í˜•)'ì—ì„œ 'Geometric(ê¸°í•˜ê¸‰ìˆ˜)'ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.
    ì´ì œ ì´ˆë°˜ì—ëŠ” ì²œì²œíˆ ë¹¨ë¼ì§€ë‹¤ê°€, í›„ë°˜ë¶€ì— ê¸‰ê²©í•˜ê²Œ(Exponential) ë¹¨ë¼ì§‘ë‹ˆë‹¤.
    """
    if start_bpm == end_bpm: return y
    
    chunk_len = len(y) // steps
    chunks = []
    
    # ğŸ”¥ [í•µì‹¬ ë³€ê²½] ê¸‰ê²©í•œ ê³¡ì„  ë§Œë“¤ê¸°
    # np.linspace (ë”í•˜ê¸° ë°©ì‹) -> np.geomspace (ê³±í•˜ê¸° ë°©ì‹)
    # ì˜ˆ: 100 -> 120 -> 150 -> 200 -> 300 -> 600 -> 2000...
    bpm_curve = np.geomspace(start_bpm, end_bpm, steps)
    
    for i in range(steps):
        start = i * chunk_len
        end = start + chunk_len if i < steps - 1 else len(y)
        chunk = y[start:end]
        
        if len(chunk) < sr * 0.05:
             chunks.append(chunk)
             continue
             
        # ë¯¸ë¦¬ ê³„ì‚°í•´ë‘” ê¸‰ê²©í•œ ê³¡ì„ ì—ì„œ ëª©í‘œ BPMì„ ê°€ì ¸ì˜´
        current_target_bpm = bpm_curve[i]
        
        # Rate ê³„ì‚°
        rate = current_target_bpm / base_bpm
        
        # Rubberband Stretch
        stretched = pyrb.time_stretch(chunk, sr, rate)
        stretched = preserve_energy(chunk, stretched)
        chunks.append(stretched)
        
    return smooth_concatenate(chunks, fade_samples=64)

def apply_high_pass(y, sr, cutoff=400):
    try:
        sos = signal.butter(10, cutoff, 'hp', fs=sr, output='sos')
        return signal.sosfilt(sos, y)
    except:
        return y

def load_and_merge_stems(track_name, stems_to_merge, output_dir, sr):
    name_no_ext = os.path.splitext(track_name)[0]
    # ëª¨ë¸ëª… í™•ì¸ (htdemucs_ft)
    demucs_path = os.path.join(output_dir, "htdemucs_ft", name_no_ext)
    merged_audio = None
    for stem in stems_to_merge:
        stem_path = os.path.join(demucs_path, f"{stem}.wav")
        if not os.path.exists(stem_path): return None
        y, _ = librosa.load(stem_path, sr=sr)
        if merged_audio is None: merged_audio = y
        else:
            min_len = min(len(merged_audio), len(y))
            merged_audio = merged_audio[:min_len] + y[:min_len]
    return merged_audio

def get_best_loop_segment(y, sr, cut_point, bpm):
    """ì—ë„ˆì§€ ê¸°ë°˜ ìµœê³ ì˜ ë¹„íŠ¸ ë£¨í”„ ì„ íƒ"""
    samples_per_beat = int(60.0 / bpm * sr)
    candidates = []
    
    for i in range(4):
        end = cut_point - (samples_per_beat * i)
        start = end - samples_per_beat
        if start < 0: break
        
        segment = y[start:end]
        rms = np.sqrt(np.mean(segment**2))
        candidates.append({'segment': segment, 'rms': rms, 'index': i})
    
    if not candidates: return None

    best_candidate = sorted(candidates, key=lambda x: x['rms'], reverse=True)[0]
    print(f"   ğŸ¯ Loop Selection: Picked beat -{best_candidate['index']+1} (Highest Energy)")
    return best_candidate['segment']

# ====================================================
# ğŸ§¨ [Strategy 1] Drop Mix Logic (Trust Vocal Stem)
# ====================================================
def run_drop_mix(y_a, y_a_vocals, y_b, bpm_a, bpm_b, sr, cut_point_a, vocal_end_point):
    print(f"\nğŸš€ [Strategy: Drop Mix] Extreme Riser Mode!")
    
    target_bpm = bpm_b * 50.0 
    print(f"   ğŸ”¥ Speed Build-up: {bpm_a:.1f} -> {target_bpm:.1f} BPM (Max 50x)")
    
    source_chunk = None
    samples_per_beat_a = int(60.0 / bpm_a * sr)
    
    # ê¸°ë³¸ ì»· í¬ì¸íŠ¸ëŠ” ê³¡ì˜ ëë¶€ë¶„ì´ì§€ë§Œ...
    actual_cut_point = cut_point_a 

    # ----------------------------------------------------
    # ğŸ”¥ [í•µì‹¬ ìˆ˜ì •] Vocal Anchor Strategy
    # ----------------------------------------------------
    # "ìŠ¤í…œì´ ì˜ ë¶„ë¦¬ë˜ì—ˆë‹¤"ë©´ vocal_end_pointëŠ” ì •í™•í•  ê²ƒì…ë‹ˆë‹¤.
    # ê³¡ì˜ ë(cut_point_a)ì—ì„œ ì°¾ì§€ ë§ê³ , vocal_end_point ì§€ì ì„ ì§ì ‘ íƒ€ê²©í•©ë‹ˆë‹¤.
    
    if vocal_end_point is not None and vocal_end_point > samples_per_beat_a:
        # ë³´ì»¬ì´ ëë‚˜ëŠ” ì§€ì  ë°”ë¡œ ì• 1ë°•ìë¥¼ ê°€ì ¸ì˜´
        vocal_chunk = y_a_vocals[vocal_end_point - samples_per_beat_a : vocal_end_point]
        
        # ì—ë„ˆì§€ í™•ì¸ (í˜¹ì‹œ ëª¨ë¥´ë‹ˆ)
        rms = np.sqrt(np.mean(vocal_chunk**2))
        print(f"   ğŸ¤ Checking Vocal End Point... RMS: {rms:.4f}")
        
        if rms > 0.001: # ì•„ì£¼ ì‘ì€ ì†Œë¦¬ë¼ë„ ìˆìœ¼ë©´ ì±„íƒ
            print("      âœ… Targeted Vocal End Point directly!")
            
            # [ë³¼ë¥¨ ë³´ì •] ìŠ¤í…œì˜ ë³¼ë¥¨ì„ ì›ê³¡ ë ˆë²¨ì— ë§ì¶¤
            full_ref = y_a[vocal_end_point - samples_per_beat_a : vocal_end_point]
            full_rms = np.sqrt(np.mean(full_ref**2))
            
            if rms > 0:
                gain = full_rms / rms
                gain = np.clip(gain, 1.5, 4.0) # ìµœëŒ€ 4ë°°ê¹Œì§€ í—ˆìš© (í™•ì‹¤í•˜ê²Œ ë“¤ë¦¬ê²Œ)
                source_chunk = vocal_chunk * gain
            else:
                source_chunk = vocal_chunk * 2.0
            
            # ğŸ”¥ [ì¤‘ìš”] ì‹¤ì œ ìë¥´ëŠ” ìœ„ì¹˜ë¥¼ 'ë³´ì»¬ì´ ëë‚˜ëŠ” ì§€ì 'ìœ¼ë¡œ ê°•ì œ ì´ë™
            # ë’¤ì— ë°˜ì£¼ê°€ ë‚¨ì•˜ì–´ë„ ë¬´ì‹œí•˜ê³  ì—¬ê¸°ì„œ ìë¦„ -> ë°”ë¡œ ë£¨í”„ ì‹œì‘
            actual_cut_point = vocal_end_point
            print(f"      âœ‚ï¸ Cut Point Moved: Syncing to Vocal End ({actual_cut_point/sr:.2f}s)")
            
    # ----------------------------------------------------
    # [Fallback] ë§Œì•½ vocal_end_pointê°€ ì´ìƒí•˜ë©´ ê¸°ì¡´ íƒìƒ‰ ë¡œì§ ê°€ë™
    # ----------------------------------------------------
    if source_chunk is None:
        print("   âš ï¸ Vocal End Point missed. Scanning backwards from instrumental end...")
        # (ê¸°ì¡´ì˜ for loop íƒìƒ‰ ë¡œì§ - ë¹„ìƒìš©)
        for i in range(16): 
            end = cut_point_a - (samples_per_beat_a * i)
            start = end - samples_per_beat_a
            if start < 0: break
            
            chunk = y_a_vocals[start:end]
            rms = np.sqrt(np.mean(chunk**2))
            if rms > 0.005:
                # ... (ë°œê²¬ ì‹œ ì²˜ë¦¬ ë¡œì§ ë™ì¼) ...
                source_chunk = chunk * 2.0 # ê°„ëµí™”
                actual_cut_point = end
                break

    # ì—¬ì „íˆ ì—†ìœ¼ë©´ ë¹„íŠ¸
    if source_chunk is None:
        print("   ğŸ¥ Fallback to Beat Loop.")
        source_chunk = get_best_loop_segment(y_a, sr, cut_point_a, bpm_a)
        if source_chunk is None:
            source_chunk = y_a[cut_point_a - samples_per_beat_a : cut_point_a]

    # ----------------------------------------------------
    # Tightening & Ramp (50x Extreme)
    # ----------------------------------------------------
    # ë£¨í”„ íƒ€ì´íŠ¸ë‹
    tight_len = int(len(source_chunk) * 0.97)
    source_chunk = source_chunk[:tight_len]

    # 12ë§ˆë”” ë°˜ë³µ
    bars = 12 
    repeats = bars * 4 
    raw_bridge = np.tile(source_chunk, repeats)
    
    # ì‹œì‘ ì†ë„ 5% ë¶€ìŠ¤íŠ¸
    adjusted_start_bpm = bpm_a * 1.05 
    
    print(f"   â±ï¸ Generating Bridge ({bars} bars)...")
    
    ramped_bridge = create_tempo_ramp(
        raw_bridge, 
        sr, 
        start_bpm=adjusted_start_bpm, 
        end_bpm=target_bpm, 
        base_bpm=bpm_a, 
        steps=repeats
    )
    
    # ì´í™íŠ¸
    filtered_bridge = apply_high_pass(ramped_bridge, sr, cutoff=400)
    fade_in = np.linspace(0.6, 1.0, len(filtered_bridge))
    final_bridge = filtered_bridge * fade_in
    
    # Track B ë¬´ìŒ ì œê±°
    y_b_trimmed, _ = librosa.effects.trim(y_b, top_db=20)

    # Hard Cut ì—°ê²°
    part_1 = smooth_concatenate([y_a[:actual_cut_point], final_bridge], fade_samples=512)
    final_mix = np.concatenate([part_1, y_b_trimmed])
    
    return final_mix

# ====================================================
# ğŸ¹ [Strategy 2] Blend Mix Logic (Padding & Timing Fix)
# ====================================================
def run_blend_mix(y_a_full, y_a_no_rhythm, y_a_vocals, y_b_full, y_b_bass, bpm_a, bpm_b, sr, 
                  overlap_samples, vocal_end, trim_point, track_b_name):
    
    print(f"\nğŸ¹ [Strategy: Blend Mix] Fixed Timing Transition...")

    # [Step 3] Processing Track B
    samples_needed_from_b = int(overlap_samples * (bpm_a / bpm_b))
    y_b_intro_raw = y_b_bass[:samples_needed_from_b]
    
    y_b_intro_raw = y_b_intro_raw * 1.5
    y_b_blend_synced = match_bpm_with_safety_margin(y_b_intro_raw, sr, bpm_b, bpm_a, overlap_samples)

    # [Step 4] Mixing
    y_a_no_bass = load_and_merge_stems(TRACK_A_NAME, ['vocals', 'drums', 'other'], OUTPUT_DIR, sr)
    
    part_a_main = y_a_full[:vocal_end]
    
    # ğŸ”¥ [ìˆ˜ì • 1] Track A ì¡°ê° ê°€ì ¸ì˜¤ê¸° (ë²”ìœ„ ì´ˆê³¼ ë°©ì§€ ë° íŒ¨ë”©)
    end_sample_a = min(len(y_a_no_bass), vocal_end + overlap_samples)
    chunk_a_raw = y_a_no_bass[vocal_end : end_sample_a]
    
    if len(chunk_a_raw) < overlap_samples:
        pad_len = overlap_samples - len(chunk_a_raw)
        chunk_a_no_bass = np.pad(chunk_a_raw, (0, pad_len))
        print(f"   âš ï¸ Track A is short. Padding {pad_len} samples to keep rhythm.")
    else:
        chunk_a_no_bass = chunk_a_raw

    # ì´ì œ ë‘ ì¡°ê°ì˜ ê¸¸ì´ëŠ” ë¬´ì¡°ê±´ overlap_samplesë¡œ ë™ì¼
    chunk_b_bass = y_b_blend_synced
    
    # ğŸ”¥ [ìˆ˜ì • 2] ì •í™•í•œ ê¸¸ì´ë¡œ ë¯¹ì‹± (min ì œê±°)
    mix_len = overlap_samples 
    
    fade_out_curve = np.linspace(1.0, 0.0, mix_len)
    mixed_chunk = (chunk_a_no_bass * fade_out_curve * 0.8) + (chunk_b_bass * 0.8)
    
    # [Step 5] Finalizing
    part_b_body = y_b_full[samples_needed_from_b:]

    transition_a_to_blend = smooth_concatenate([part_a_main, mixed_chunk], fade_samples=512)
    
    # ğŸ”¥ ì •í™•í•œ íƒ€ì´ë° ì—°ê²° (Micro-Fade)
    final_mix = smooth_concatenate([transition_a_to_blend, part_b_body], fade_samples=256)
    
    return final_mix

# ====================================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰ ë¡œì§
# ====================================================
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    file_a = os.path.join(INPUT_DIR, TRACK_A_NAME)
    file_b = os.path.join(INPUT_DIR, TRACK_B_NAME)

    if not os.path.exists(file_a) or not os.path.exists(file_b):
        print("âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\nğŸ§ Mixing Track A: {TRACK_A_NAME}")
    print(f"ğŸ§ Mixing Track B: {TRACK_B_NAME}")

    print("\n[Step 0] Preparing Stems...")
    separate_stems(TRACK_A_NAME)
    separate_stems(TRACK_B_NAME)

    y_a_full, sr = librosa.load(file_a, sr=TARGET_SR)
    y_b_full, _ = librosa.load(file_b, sr=TARGET_SR)

    y_a_no_rhythm = load_and_merge_stems(TRACK_A_NAME, ['vocals', 'other'], OUTPUT_DIR, TARGET_SR)
    y_b_rhythm = load_and_merge_stems(TRACK_B_NAME, ['bass', 'drums'], OUTPUT_DIR, TARGET_SR)
    y_a_vocals_only = load_and_merge_stems(TRACK_A_NAME, ['vocals'], OUTPUT_DIR, TARGET_SR)
    y_b_bass_only = load_and_merge_stems(TRACK_B_NAME, ['bass'], OUTPUT_DIR, TARGET_SR)

    print("\n[Step 1] Analyzing Audio...")
    info_a = get_beat_info(file_a)
    info_b = get_beat_info(file_b)
    bpm_a, bpm_b = info_a['bpm'], info_b['bpm']
    
    bpm_diff = abs(bpm_a - bpm_b)
    print(f"\nâš–ï¸ BPM Difference: {bpm_diff:.1f} (Threshold: {BPM_THRESHOLD})")

    trim_point_vol = find_outro_endpoint(y_a_full, sr)
    snapped_point = find_smart_trim_point(y_a_full, sr, trim_point_vol, bpm_a)
    final_trim_point = snapped_point

    # ğŸ”¥ ê³µí†µ: ë³´ì»¬ ëë‚˜ëŠ” ì§€ì  ê³„ì‚° (Drop Mixì—ì„œë„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ìœ„ë¡œ ì´ë™)
    vocal_end_point = find_vocal_end_point(y_a_vocals_only, sr)

    if bpm_diff > BPM_THRESHOLD:
        final_mix = run_drop_mix(
            y_a=y_a_full,
            y_a_vocals=y_a_vocals_only, # ë³´ì»¬ ìŠ¤í…œ ì „ë‹¬
            y_b=y_b_full,
            bpm_a=bpm_a,
            bpm_b=bpm_b,
            sr=sr,
            cut_point_a=final_trim_point,
            vocal_end_point=vocal_end_point # ë³´ì»¬ ë ì§€ì  ì „ë‹¬
        )
        strategy_name = "drop_mix"
    else:
        intro_sec_raw_b = get_intro_duration(file_b)
        intro_beats = max(4, int(round(intro_sec_raw_b * (bpm_b / 60.0))))
        overlap_duration_target = intro_beats * (60.0 / bpm_a)
        overlap_samples_target = int(overlap_duration_target * sr)
        
        # í‚¤ ë§¤ì¹­
        key_a, _ = get_key_from_audio(y_a_full, sr)
        key_b, _ = get_key_from_audio(y_b_bass_only, sr)
        shift_steps = get_pitch_shift_steps(key_a, key_b)
        if shift_steps != 0:
            print(f"   ğŸ¹ Auto Pitch Shift: {shift_steps} semitones")
            y_b_bass_only = pyrb.pitch_shift(y_b_bass_only, sr, n_steps=shift_steps)

        final_mix = run_blend_mix(
            y_a_full=y_a_full,
            y_a_no_rhythm=y_a_no_rhythm,
            y_a_vocals=y_a_vocals_only,
            y_b_full=y_b_full,
            y_b_bass=y_b_bass_only,
            bpm_a=bpm_a,
            bpm_b=bpm_b,
            sr=sr,
            overlap_samples=overlap_samples_target,
            vocal_end=vocal_end_point,
            trim_point=final_trim_point,
            track_b_name=TRACK_B_NAME
        )
        strategy_name = "blend_mix"

    final_mix = normalize_audio(final_mix)
    name_a = os.path.splitext(TRACK_A_NAME)[0]
    name_b = os.path.splitext(TRACK_B_NAME)[0]
    output_path = os.path.join(OUTPUT_DIR, f"mix_{strategy_name}_{name_a}_to_{name_b}.wav")
    
    sf.write(output_path, final_mix, sr)
    print(f"\nâœ¨ Success! [{strategy_name}] saved to: {output_path}")

if __name__ == "__main__":
    main()